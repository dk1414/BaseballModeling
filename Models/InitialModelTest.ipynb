{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Datasets.BaseballDataset import BaseballDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_config_path = \"../data/config.json\"\n",
    "valid_path = \"../data/statcast_2023-2024_cleaned.csv\"\n",
    "valid_data = pd.read_csv(valid_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../data/config.json\"\n",
    "sequence_length = 20 \n",
    "\n",
    "\n",
    "valid_dataset = BaseballDataset(valid_data,config,sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_encoder_layers, hidden_dim, output_dim, sequence_length, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim, dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]  # Use the output of the last pitch in the sequence\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weight_param):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weight_param = weight_param\n",
    "\n",
    "    def forward(self, output, target_continuous, target_categorical):\n",
    "        # Continuous target loss (MSE)\n",
    "        mse_loss = F.mse_loss(output[:, :target_continuous.size(1)], target_continuous)\n",
    "        \n",
    "        # Categorical target loss (Cross-Entropy) for each categorical feature\n",
    "        cross_entropy_loss = 0\n",
    "        start_idx = target_continuous.size(1)\n",
    "        for cat_target in target_categorical:\n",
    "            end_idx = start_idx + cat_target.size(1)\n",
    "            cross_entropy_loss += F.cross_entropy(output[:, start_idx:end_idx], cat_target.argmax(dim=1))\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        # # Sum of all categorical losses\n",
    "        # cross_entropy_loss = torch.sum(categorical_losses)\n",
    "\n",
    "        # Weighted sum of the losses\n",
    "        loss = (self.weight_param * mse_loss) + ((1 - self.weight_param) * cross_entropy_loss)\n",
    "        return loss\n",
    "\n",
    "# # Hyperparameters\n",
    "# input_dim = 65 # Number of features in a single pitch\n",
    "# num_heads = 4\n",
    "# num_encoder_layers = 4\n",
    "# hidden_dim = 40 # Increased hidden dimension for better representation\n",
    "# output_dim = 6  # Number of label dimensions\n",
    "# sequence_length = 20\n",
    "# dropout = 0.1\n",
    "# batch_size = 32\n",
    "\n",
    "# # Initialize the model, loss function, and optimizer\n",
    "# model = TransformerModel(input_dim, num_heads, num_encoder_layers, hidden_dim, output_dim, sequence_length, dropout)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Dummy data for testing\n",
    "# dummy_input = torch.randn(batch_size, sequence_length, input_dim)\n",
    "# dummy_target_continuous = torch.randn(batch_size, 3)  # Assuming 3 continuous targets\n",
    "# dummy_target_categorical = torch.randint(0, 2, (batch_size, 3))  # Assuming 3 categorical targets\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(dummy_input)\n",
    "\n",
    "# # Define the custom loss function\n",
    "# weight_param = 0.5  # Adjust this weight parameter as needed\n",
    "# criterion = CustomLoss(weight_param)\n",
    "\n",
    "# # Compute the loss\n",
    "# loss = criterion(output, dummy_target_continuous, dummy_target_categorical)\n",
    "# print(f'Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 75])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
       " tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\declan\\anaconda3\\envs\\pytorchCUDA\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1/2, Train Loss: 0.6349, Val Loss: 0.5596\n",
      "Epoch 2/2, Train Loss: 0.5965, Val Loss: 0.5538\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim = 75  # Number of features in a single pitch\n",
    "num_heads = 1\n",
    "num_encoder_layers = 1\n",
    "hidden_dim = 30\n",
    "output_dim = 24 # Number of label dimensions\n",
    "sequence_length = 20\n",
    "dropout = 0.1\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TransformerModel(input_dim, num_heads, num_encoder_layers, hidden_dim, output_dim, sequence_length, dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CustomLoss(weight_param=0.5)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for sequences, cont_targets, cat_targets in train_loader:\n",
    "        sequences, cont_targets = sequences.to(device), cont_targets.to(device)\n",
    "        cat_targets = [t.to(device) for t in cat_targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, cont_targets, cat_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sequences, cont_targets, cat_targets in val_loader:\n",
    "            sequences, cont_targets = sequences.to(device), cont_targets.to(device)\n",
    "            cat_targets = [t.to(device) for t in cat_targets]\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, cont_targets, cat_targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.stack([torch.tensor([1,2,3]),torch.tensor([4,5,6]),torch.tensor([7,8,9])])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(a,0,torch.tensor([1,2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaseballModeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
